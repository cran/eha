<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Göran Broström" />

<meta name="date" content="2024-09-19" />

<title>Event History and Survival Analysis</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/rstudio/markdown/inst/resources/prism-xcode.css" data-external="1">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/yihui/knitr/inst/misc/vignette.css" data-external="1">
<script src="https://cdn.jsdelivr.net/combine/npm/@xiee/utils/js/code-lang.min.js,npm/@xiee/utils/js/number-captions.min.js,npm/prismjs@1.29.0/components/prism-core.min.js" data-external="1" defer></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" data-external="1" defer></script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>






<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>





</head>

<body>




<h1 class="title toc-ignore">Event History and Survival Analysis</h1>
<h3 class="subtitle">The eha package</h3>
<h4 class="author">Göran Broström</h4>
<h4 class="date">2024-09-19</h4>


<div id="TOC">
<ul>
<li><a href="#background" id="toc-background">Background</a>
<ul>
<li><a href="#early-eighties" id="toc-early-eighties">Early
eighties</a></li>
<li><a href="#extensions-to-cox-regression" id="toc-extensions-to-cox-regression">Extensions to Cox regression</a>
<ul>
<li><a href="#relevant-printed-output" id="toc-relevant-printed-output">Relevant printed output</a></li>
<li><a href="#sampling-of-risk-sets" id="toc-sampling-of-risk-sets">Sampling of risk sets</a></li>
<li><a href="#the-weird-bootstrap" id="toc-the-weird-bootstrap">The
weird bootstrap</a></li>
<li><a href="#discrete-time-methods" id="toc-discrete-time-methods">Discrete time methods</a></li>
</ul></li>
<li><a href="#later-development" id="toc-later-development">Later
development</a></li>
</ul></li>
<li><a href="#parametric-survival-models" id="toc-parametric-survival-models">Parametric survival models</a>
<ul>
<li><a href="#accelerated-failure-time-aft-models" id="toc-accelerated-failure-time-aft-models">Accelerated Failure Time
(AFT) models</a></li>
<li><a href="#proportional-hazards-ph-models" id="toc-proportional-hazards-ph-models">Proportional Hazards (PH)
models</a></li>
<li><a href="#ph-models-with-tabular-data" id="toc-ph-models-with-tabular-data">PH models with tabular
data</a></li>
<li><a href="#the-gompertz-distribution" id="toc-the-gompertz-distribution">The Gompertz distribution</a></li>
</ul></li>
<li><a href="#utilities" id="toc-utilities">Utilities</a>
<ul>
<li><a href="#lexis-cuts" id="toc-lexis-cuts">Lexis cuts</a></li>
<li><a href="#tabulation" id="toc-tabulation">Tabulation</a></li>
</ul></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</div>

<p>This vignette is still ongoing work, so if you are looking for
something you cannot find, please <a href="https://github.com/goranbrostrom/eha/issues/">alert me</a> and I
will do something about it.</p>
<div id="background" class="section level1">
<h1>Background</h1>
<div id="early-eighties" class="section level2">
<h2>Early eighties</h2>
<p>In the year 1979, I got my first real job after finishing my PhD
studies in mathematical statistics at Umeå University the same year. My
new job was as a statistical consultant at the Demographic Data Base
(DDB), Umeå University, and I soon got involved in a research project
concerning infant mortality in the 19th century northern Sweden.
Individual data were collected from church books registered at the DDB,
and we searched for relevant statistical methods for the analysis of the
data in relation to our research questions.</p>
<p>Almost parallel in time, the now classical book <em>“The Statistical
Analysis of Failure Time Data”</em>, <span class="citation">Kalbfleisch
and Prentice (1980)</span> was published, and I realized that I had
found the answer to our prayers. There was only one small problem, lack
of suitable software. However, there was in the book an appendix with
Fortran code for <em>Cox regression</em>, and since I had taken a course
in Fortran77, I decided to transfer the appendix to to punch cards(!)
and feed them to the mainframe at the university, of course with our
infant mortality data. Bad luck: It turned out that by accident two
pages in the appendix had been switched, without introducing any
syntactic error! It however introduced an infinite loop in the code, so
the prize for the first run was high (this error was corrected in later
printings of the book).</p>
<p>This was the starting point of my development of software for
survival analysis. The big challenge was to find ways to illustrate
results graphically. It led to translating the Fortran code to <em>Turbo
Pascal</em> (Borland, MS Dos) in the mid and late eighties.</p>
<p>I soon regretted that choice, and the reason was
<em>portability</em>: That Pascal code didn’t work well on Unix work
stations and other environments outside MS Dos. And on top of that,
another possibility for getting graphics into the picture showed soon
up: <strong>R</strong>. So the Pascal code was quickly translated into
<em>C</em>, and it was an easy process to call the C and Fortran
functions from R and on top of that write simple routines for presenting
results graphically and as tables. And so in 2003, the <em>eha</em>
package was introduced on <em>CRAN</em>.</p>
</div>
<div id="extensions-to-cox-regression" class="section level2">
<h2>Extensions to Cox regression</h2>
<p>During the eighties and nineties, focus was on <em>Cox
regression</em> and extensions thereof, but after the transform to an R
package, the <em>survival</em> package has been allowed to take over
most of the stuff that was (and still is) found in the
<strong>eha::coxreg</strong> function.</p>
<p>Regarding <em>Cox regression</em>, the <strong>eha</strong> package
can be seen as a complement to the recommended package
<strong>survival</strong>: In fact, <strong>eha</strong>
<em>imports</em> some functions from <strong>survival</strong>, and for
<em>standard Cox regression</em>, <code>eha::coxreg()</code> simply
calls <code>survival::agreg.fit()</code> or
<code>survival::coxph.fit()</code>, functions <em>exported</em> by
<strong>survival</strong>. The simple reason for this is that the
underlying code in these <strong>survival</strong> functions is very
fast and efficient. However, <code>eha::coxreg()</code> has some unique
features: <em>Sampling of risk sets</em>, <em>The “weird”
bootstrap</em>, and <em>discrete time modeling</em> via maximum
likelihood, which motivates the continued support of it.</p>
<p>I have put effort in producing nice and relevant printouts of
regression results, both on screen and to <span class="math inline">\(\LaTeX\)</span> documents (HTML output may come
next). By <em>relevant</em> output I basically mean <em>avoiding
misleading p-values</em>, show all <em>factor levels</em>, and use the
<em>likelihood ratio test</em> instead of the <em>Wald test</em> where
possible.</p>
<p>To summarize, the extensions are (in descending order of importance,
by my own judgement).</p>
<div id="relevant-printed-output" class="section level3">
<h3>Relevant printed output</h3>
<p>Consider the following standard way of performing a Cox regression
and reporting the result.</p>
<pre class="r"><code>fit &lt;- survival::coxph(Surv(enter, exit, event) ~ sex + civ + imr.birth, 
                       data = oldmort)
summary(fit)</code></pre>
<pre><code>## Call:
## survival::coxph(formula = Surv(enter, exit, event) ~ sex + civ + 
##     imr.birth, data = oldmort)
## 
##   n= 6495, number of events= 1971 
## 
##                coef exp(coef) se(coef)     z Pr(&gt;|z|)    
## sexfemale  -0.24306   0.78423  0.04742 -5.13  3.0e-07 ***
## civmarried -0.39549   0.67335  0.08128 -4.87  1.1e-06 ***
## civwidow   -0.25980   0.77120  0.07882 -3.30  0.00098 ***
## imr.birth   0.00283   1.00284  0.00634  0.45  0.65487    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##            exp(coef) exp(-coef) lower .95 upper .95
## sexfemale      0.784      1.275     0.715     0.861
## civmarried     0.673      1.485     0.574     0.790
## civwidow       0.771      1.297     0.661     0.900
## imr.birth      1.003      0.997     0.990     1.015
## 
## Concordance= 0.55  (se = 0.008 )
## Likelihood ratio test= 41.4  on 4 df,   p=2e-08
## Wald test            = 42.6  on 4 df,   p=1e-08
## Score (logrank) test = 42.7  on 4 df,   p=1e-08</code></pre>
<p>The presentation of the covariates and corresponding coefficient
estimates follows common <strong>R</strong> standard, but it is a little
bit confusing regarding covariates that are represented as
<em>factors</em>. In this example we have <em>civ</em> (civil status),
with three levels, <code>unmarried</code>, <code>married</code>, and
<code>widow</code>, but only two are shown, the <em>reference
category</em>, <code>unmarried</code>, is hidden. Moreover, the variable
<em>name</em> is joined with the variable <em>labels</em>, in a somewhat
hard-to-read fashion. I prefer the following result presentation.</p>
<pre class="r"><code>fit &lt;- coxreg(Surv(enter, exit, event) ~ sex + civ + imr.birth, data = oldmort)
summary(fit)</code></pre>
<pre><code>## Covariate             Mean       Coef     Rel.Risk   S.E.    LR p
## sex                                                          0.000 
##             male      0.406     0         1 (reference)
##           female      0.594    -0.243     0.784     0.047
## civ                                                          0.000 
##        unmarried      0.080     0         1 (reference)
##          married      0.530    -0.395     0.673     0.081
##            widow      0.390    -0.260     0.771     0.079
## imr.birth            15.162     0.003     1.003     0.006    0.655 
## 
## Events                    1971 
## Total time at risk         37824 
## Max. log. likelihood      -13558 
## LR test statistic         41.43 
## Degrees of freedom        4 
## Overall p-value           2.18642e-08</code></pre>
<p>There are a few things to notice here. First, the layout: Variables
are clearly separated from their labels, and <em>all</em> categories for
factors are shown, even the reference categories. Second,
<em>p</em>-values are given for <em>variables</em>, not for levels, and
third, the <em>p</em>-values are <em>likelihood ratio</em> (LR) based,
and not Wald tests. The importance of this was explained by <span class="citation">Hauck and Donner (1977)</span>. They considered
logistic regression, but their conclusions in fact hold for most
nonlinear models. The very short version is that a large Wald
<em>p</em>-value can mean one of two things: (i) Your finding is
statistically non-significant (what you expect), or (ii) your finding is
strongly significant (surprise!). Experience shows that condition (ii)
is quite rare, but why take chances? (The truth is that it is more time
consuming and slightly more complicated to program, so standard
statistical software, including <strong>R</strong>, avoids it.)</p>
<p>Regarding <em>p</em>-values for <em>levels</em>, it is a strongly
discouraged practice. <em>Only</em> if a test shows a small enough
<em>p</em>-value for the <em>variable</em>, dissemination of the
significance of contrasts is allowed, and then only if treated as a
<em>mass significance</em> problem .</p>
</div>
<div id="sampling-of-risk-sets" class="section level3">
<h3>Sampling of risk sets</h3>
<p>The idea is that you can regard what happens in a risk set as a very
unbalanced two-sample problem: The two groups are (i) those who dies at
the given time point, often only one individual, and (ii) those who
survive (many persons). The questions is which covariate values that
tend to create a death, and we simply can do without so many survivors,
so we take a random sample of them. It will save computer time and
storage, and in some cases the prize for collecting the information
about all survivors is high <span class="citation">(Borgan, Goldstein,
and Langholz 1995)</span>.</p>
</div>
<div id="the-weird-bootstrap" class="section level3">
<h3>The weird bootstrap</h3>
<p>The weird bootstrap is aimed at getting estimates of the uncertainty
of the regression parameter estimates in a Cox regression. It works by
regarding each risk set and the number of failures therein as given and
by simulation determining who will be failures. It is assumed that what
happens in one riskset is independent of what has happened in other risk
sets (this is the “weird” part in the procedure). This is repeated many
times, resulting in a collection of parameter estimates, the bootstrap
samples.</p>
<pre class="r"><code>library(eha)
fit &lt;- coxreg(Surv(enter, exit, event) ~ sex, data = oldmort, boot = 100, 
              control = list(trace = TRUE))</code></pre>
</div>
<div id="discrete-time-methods" class="section level3">
<h3>Discrete time methods</h3>
<p>These methods are supposed to be used with data that are heavily
tied, so that a discrete time model may be reasonable.</p>
<p>The method <em>ml</em> performs a maximum likelihood estimation, and
the “mppl” method is a compromise between the ML method and Efron’s
metod of handling ties. These methods are described in detail in <span class="citation">Broström (2002)</span>.</p>
<pre class="r"><code>library(eha)
fit &lt;- coxreg(Surv(enter, exit, event) ~ sex, data = oldmort, method = &quot;ml&quot;)
plot(c(60, fit$hazards[[1]][, 1]), c(0, cumsum(fit$hazards[[1]][, 2])), type = &quot;l&quot;)</code></pre>
<p>The importance of discrete time methods in the framework of Cox
regression, as a means of treating tied data, has diminished in the
light of <em>Efron</em>’s approximation, today the default method in the
<em>survival</em> and <em>eha</em> packages.</p>
</div>
</div>
<div id="later-development" class="section level2">
<h2>Later development</h2>
<p>The development before 2010 is summarized in the book <em>Event
History Analysis with R</em> <span class="citation">(Broström
2012)</span>. Later focus has been on parametric survival models, who
are more suitable to handle huge amounts of data through methods based
on the theory of sufficient statistics, in particular <em>piecewise
constant hazards models</em>. In demographic applications (in particular
<em>mortality</em> studies), the <em>Gompertz</em> survival distribution
is important, because adult mortality universally shows a pattern of
increasing exponentially with increasing age.</p>
</div>
</div>
<div id="parametric-survival-models" class="section level1">
<h1>Parametric survival models</h1>
<p>There is a special vignette describing the theory and implementation
of the parametric failure time models. It is <em>not</em> very useful as
a <em>user’s manual</em>. It also has the flaw that it only considers
the theory in the case of time fixed covariates, although it also works
for time-varying ones. This is fairy trivial for the PH models, but some
care is needed to be taken with the AFT models.</p>
<div id="accelerated-failure-time-aft-models" class="section level2">
<h2>Accelerated Failure Time (AFT) models</h2>
<p>The parametric accelerated failure time (AFT) models are present via
<code>eha::aftreg()</code>, which is corresponding to
<code>survival::survreg()</code>. An important difference is that
<code>eha::aftreg()</code> allows for <em>left truncated data</em>.</p>
</div>
<div id="proportional-hazards-ph-models" class="section level2">
<h2>Proportional Hazards (PH) models</h2>
<p>Parametric proportional hazards (PH) modeling is available through
the functions <code>eha::phreg()</code> and <code>eha::weibreg()</code>,
the latter still in the package for historical reasons. It will
eventually be removed, since the Weibull distribution is also available
in <code>eha::phreg()</code>.</p>
</div>
<div id="ph-models-with-tabular-data" class="section level2">
<h2>PH models with tabular data</h2>
<p>Cox regression is not very suitable in the analysis of huge data sets
with a lot of events (e.g., deaths). For instance, consider analyzing
the mortality of the Swedish population aged 60–110 during the years
1968-2019, where we can count to more than four million deaths. This is
elaborated in a separate vignette.</p>
</div>
<div id="the-gompertz-distribution" class="section level2">
<h2>The Gompertz distribution</h2>
<p>The <em>Gompertz</em> distribution has long been part of the possible
distributions in the <code>phreg</code> and <code>aftreg</code>
functions, but it will be placed in a function of its own,
<code>gompreg</code>, see the separate vignette on this topic.</p>
</div>
</div>
<div id="utilities" class="section level1">
<h1>Utilities</h1>
<p>The primary applications in mind for <strong>eha</strong> were
<em>demography</em> and <em>epidemiology</em>. There are some functions
in <strong>eha</strong> that makes certain common tasks in that context
easy to perform, for instance <em>rectangular cuts</em> in the <em>Lexis
diagram</em>, creating <em>period</em> and <em>cohort</em> statistics,
etc.</p>
<div id="lexis-cuts" class="section level2">
<h2>Lexis cuts</h2>
</div>
<div id="tabulation" class="section level2">
<h2>Tabulation</h2>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-bgl95" class="csl-entry">
Borgan, Ø., L. Goldstein, and B. Langholz. 1995. <span>“Methods for the
Analysis of Sampled Cohort Data in the <span>C</span>ox Proportional
Hazards Model.”</span> <em>The Annals of Statistics</em> 23: 1749–78.
</div>
<div id="ref-gb02" class="csl-entry">
Broström, G. 2002. <span>“Cox Regression: <span>T</span>ies Without
Tears.”</span> <em>Communications in Statistics: Theory and Methods</em>
31: 285–97.
</div>
<div id="ref-gb12" class="csl-entry">
———. 2012. <em>Event History Analysis with r</em>. Boca Raton: Chapman
&amp; Hall/CRC.
</div>
<div id="ref-hd77" class="csl-entry">
Hauck, W. W., and A. Donner. 1977. <span>“Wald’s Test as Applied to
Hyptheses in Logit Analysis.”</span> <em>Journal of the American
Statistical Association</em> 72: 851–53.
</div>
<div id="ref-kp80" class="csl-entry">
Kalbfleisch, J. D., and R. L. Prentice. 1980. <em>The Statistical
Analysis of Failure Time Data</em>. <span>F</span>irst. Hoboken, N.J.:
Wiley.
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
